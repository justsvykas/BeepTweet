{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af9f4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is supposed to represent how I chose between\n",
    "# Linear Regression and Ridge regression\n",
    "# And between different preprocessings ngram and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da7150e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eb = pd.read_csv(\"emobank.csv\", index_col=0 ,engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a59082e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb.reset_index(drop=True, inplace=True)\n",
    "eb = eb.drop(labels='split', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83cc7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_list = eb['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3460bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract VAD values\n",
    "y_V = eb['V']\n",
    "y_A = eb['A']\n",
    "y_D = eb['D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc8e94d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:438: MarkupResemblesLocatorWarning: \"http://www.law.com/jsp/nlj/PubArticleNLJ.jsp?id=1202463630848&Alcohol_bill_means_happy_hour_for_lobbyists\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "c:\\users\\justs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:343: MarkupResemblesLocatorWarning: \".\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "# Based on https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "# Data cleaning\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "testing = eb_list\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad13fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_list_V = test_result\n",
    "eb_list_A = test_result\n",
    "eb_list_D = test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4418f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature \n",
    "# Preprocessing function with Tf-idf\n",
    "\n",
    "# Based on code from:\n",
    "# https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts. This way no leaking of information is done.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98c9f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "# This function is taken from \n",
    "# https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "becd4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_Vtrain, X_Vtest, y_Vtrain, y_Vtest = train_test_split(eb_list_V, y_V, test_size=0.2)\n",
    "X_Atrain, X_Atest, y_Atrain, y_Atest = train_test_split(eb_list_A, y_A, test_size=0.2)\n",
    "X_Dtrain, X_Dtest, y_Dtrain, y_Dtest = train_test_split(eb_list_D, y_D, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e70bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data with ngram\n",
    "X_Vtrain, X_Vtest = ngram_vectorize(X_Vtrain, y_Vtrain, X_Vtest)\n",
    "X_Atrain, X_Atest = ngram_vectorize(X_Atrain, y_Atrain, X_Atest)\n",
    "X_Dtrain, X_Dtest = ngram_vectorize(X_Dtrain, y_Dtrain, X_Dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df435b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into dev set from training set so we get the same distribution\n",
    "X_Vtrain, X_Vdev, y_Vtrain, y_Vdev = train_test_split(X_Vtrain, y_Vtrain, test_size=0.18)\n",
    "X_Atrain, X_Adev, y_Atrain, y_Adev = train_test_split(X_Atrain, y_Atrain, test_size=0.18)\n",
    "X_Dtrain, X_Ddev, y_Dtrain, y_Ddev = train_test_split(X_Dtrain, y_Dtrain, test_size=0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd2db9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data with embeddings\n",
    "#X_Vtrain, X_Vtest, word_index_V = sequence_vectorize(X_Vtrain, X_Vtest)\n",
    "#X_Atrain, X_Atest, word_index_A = sequence_vectorize(X_Atrain, X_Atest)\n",
    "#X_Dtrain, X_Dtest, word_index_D = sequence_vectorize(X_Dtrain, X_Dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d1adaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into dev set from training set so we get the same distribution\n",
    "#X_Vtrain, X_Vdev, y_Vtrain, y_Vdev = train_test_split(X_Vtrain, y_Vtrain, test_size=0.18)\n",
    "#X_Atrain, X_Adev, y_Atrain, y_Adev = train_test_split(X_Atrain, y_Atrain, test_size=0.18)\n",
    "#X_Dtrain, X_Ddev, y_Dtrain, y_Ddev = train_test_split(X_Dtrain, y_Dtrain, test_size=0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79556500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATASET\n",
      "V - LR\n",
      "[ 7.54925014e+03 -2.57138595e+02  3.10092267e+00 ... -1.15761911e+03\n",
      "  4.32538116e+02  5.64990252e+02]\n",
      "A - LR\n",
      "[2.84115232e+00 8.53248467e+02 6.23719497e+03 ... 3.60925459e+02\n",
      " 5.17700904e+02 3.62462686e+03]\n",
      "D - LR\n",
      "[3.09459608 3.70540558 3.05120309 ... 2.97981816 2.71343974 3.69326539]\n",
      "V - Ridge\n",
      "[2.8708389 2.8326027 3.0746799 ... 2.9857569 3.0555685 2.7819338]\n",
      "A - Ridge\n",
      "[2.9972575 3.075153  3.3334167 ... 2.9729784 3.0296562 3.0360131]\n",
      "D - Ridge\n",
      "[3.0842304 3.1119394 3.0998728 ... 2.987238  3.0038261 3.0756059]\n",
      "DEV DATASET\n",
      "V - LR\n",
      "[-3303.26815626  1550.70937979 -2815.38536618 ...  3372.90514243\n",
      " -5900.76834704  -149.7770566 ]\n",
      "A - LR\n",
      "[ 4350.88992841  -777.62882734  -176.94656057 ...  6884.23581085\n",
      " 11593.66775549 -4495.20836848]\n",
      "D - LR\n",
      "[3.0249512  3.18987934 3.03679531 ... 3.20040145 2.49181865 1.93669366]\n",
      "V - Ridge\n",
      "[3.0451865 2.907773  2.669166  ... 3.1469455 2.9960103 3.193222 ]\n",
      "A - Ridge\n",
      "[2.9835598 3.1777723 2.9957523 ... 3.0401835 3.0368035 3.1670716]\n",
      "D - Ridge\n",
      "[3.0169127 3.1549304 2.9965088 ... 3.0732486 2.8897276 3.0421176]\n",
      "TRAIN DATASET\n",
      "V - LR\n",
      "[2.90375273 3.00230416 3.00468613 ... 3.19864086 2.59949484 3.00087754]\n",
      "A - LR\n",
      "[2.3994939  2.86478505 3.00302881 ... 2.99547546 3.69619068 3.10575298]\n",
      "D - LR\n",
      "[3.19999993 3.36999988 3.09999981 ... 2.7999998  2.99999991 3.        ]\n",
      "V - Ridge\n",
      "[2.9840157 2.9009235 3.0122435 ... 3.1578386 2.6572316 3.0203478]\n",
      "A - Ridge\n",
      "[2.68469   2.9961092 3.041916  ... 3.1263292 3.3453794 3.1279862]\n",
      "D - Ridge\n",
      "[3.1355782 3.0992103 3.0986655 ... 2.9248273 3.0118167 3.10844  ]\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "from sklearn.linear_model import Ridge# Classifier: \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "clf1 = LinearRegression()\n",
    "clf2 = LinearRegression()\n",
    "clf3 = LinearRegression()\n",
    "clf4 = Ridge()\n",
    "clf5 = Ridge()\n",
    "clf6 = Ridge()\n",
    "\n",
    "# train model on data\n",
    "clf1 = clf1.fit(X_Vtrain, y_Vtrain) \n",
    "clf2 = clf2.fit(X_Atrain, y_Atrain)\n",
    "clf3 = clf3.fit(X_Dtrain, y_Dtrain) \n",
    "clf4 = clf4.fit(X_Vtrain, y_Vtrain) \n",
    "clf5 = clf5.fit(X_Atrain, y_Atrain)\n",
    "clf6 = clf6.fit(X_Dtrain, y_Dtrain) \n",
    "\n",
    "print(\"TEST DATASET\")\n",
    "rslt_V_L = clf1.predict(X_Vtest)\n",
    "print('V - LR')\n",
    "print(rslt_V_L)\n",
    "rslt_A_L = clf2.predict(X_Atest)\n",
    "print('A - LR')\n",
    "print(rslt_A_L)\n",
    "rslt_D_L = clf3.predict(X_Dtest)\n",
    "print('D - LR')\n",
    "print(rslt_D_L)\n",
    "rslt_V_R = clf4.predict(X_Vtest)\n",
    "print('V - Ridge')\n",
    "print(rslt_V_R)\n",
    "rslt_A_R = clf5.predict(X_Atest)\n",
    "print('A - Ridge')\n",
    "print(rslt_A_R)\n",
    "rslt_D_R = clf6.predict(X_Dtest)\n",
    "print('D - Ridge')\n",
    "print(rslt_D_R)\n",
    "\n",
    "print(\"DEV DATASET\")\n",
    "rslt_dev_V_L = clf1.predict(X_Vdev)\n",
    "print('V - LR')\n",
    "print(rslt_dev_V_L)\n",
    "rslt_dev_A_L = clf2.predict(X_Adev)\n",
    "print('A - LR')\n",
    "print(rslt_dev_A_L)\n",
    "rslt_dev_D_L = clf3.predict(X_Ddev)\n",
    "print('D - LR')\n",
    "print(rslt_dev_D_L)\n",
    "rslt_dev_V_R = clf4.predict(X_Vdev)\n",
    "print('V - Ridge')\n",
    "print(rslt_dev_V_R)\n",
    "rslt_dev_A_R = clf5.predict(X_Adev)\n",
    "print('A - Ridge')\n",
    "print(rslt_dev_A_R)\n",
    "rslt_dev_D_R = clf6.predict(X_Ddev)\n",
    "print('D - Ridge')\n",
    "print(rslt_dev_D_R)\n",
    "\n",
    "print(\"TRAIN DATASET\")\n",
    "rslt_train_V_L = clf1.predict(X_Vtrain)\n",
    "print('V - LR')\n",
    "print(rslt_train_V_L)\n",
    "rslt_train_A_L = clf2.predict(X_Atrain)\n",
    "print('A - LR')\n",
    "print(rslt_train_A_L)\n",
    "rslt_train_D_L = clf3.predict(X_Dtrain)\n",
    "print('D - LR')\n",
    "print(rslt_train_D_L)\n",
    "rslt_train_V_R = clf4.predict(X_Vtrain)\n",
    "print('V - Ridge')\n",
    "print(rslt_train_V_R)\n",
    "rslt_train_A_R = clf5.predict(X_Atrain)\n",
    "print('A - Ridge')\n",
    "print(rslt_train_A_R)\n",
    "rslt_train_D_R = clf6.predict(X_Dtrain)\n",
    "print('D - Ridge')\n",
    "print(rslt_train_D_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da6522b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression - test dataset\n",
      "Mean squared error for Valance: 0.09\n",
      "Mean squared error for Arousal: 0.06\n",
      "Mean squared error for Dominance: 0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge regression - test dataset\")\n",
    "print(\"Mean squared error for Valance: %.2f\" % mean_squared_error(y_Vtest, rslt_V_R))\n",
    "#print(\"Coefficient of determination Valence: %.2f\" % r2_score(y_Vtest, rslt_V_R))\n",
    "print(\"Mean squared error for Arousal: %.2f\" % mean_squared_error(y_Atest, rslt_A_R))\n",
    "#print(\"Coefficient of determination Arousal: %.2f\" % r2_score(y_Atest, rslt_A_R))\n",
    "print(\"Mean squared error for Dominance: %.2f\" % mean_squared_error(y_Dtest, rslt_D_R))\n",
    "#print(\"Coefficient of determination Dominance: %.2f\" % r2_score(y_Dtest, rslt_D_R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ab940e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression - test dataset\n",
      "Mean squared error for Valance: 4345915.01\n",
      "Mean squared error for Arousal: 40398304.30\n",
      "Mean squared error for Dominance: 0.10\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear regression - test dataset\")\n",
    "print(\"Mean squared error for Valance: %.2f\" % mean_squared_error(y_Vtest, rslt_V_L))\n",
    "#print(\"Coefficient of determination Valence: %.2f\" % r2_score(y_Vtest, rslt_V))\n",
    "print(\"Mean squared error for Arousal: %.2f\" % mean_squared_error(y_Atest, rslt_A_L))\n",
    "#print(\"Coefficient of determination Arousal: %.2f\" % r2_score(y_Atest, rslt_A))\n",
    "print(\"Mean squared error for Dominance: %.2f\" % mean_squared_error(y_Dtest, rslt_D_L))\n",
    "#print(\"Coefficient of determination Dominance: %.2f\" % r2_score(y_Dtest, rslt_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ebf4403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression - train dataset\n",
      "Mean squared error for Valance: 0.03\n",
      "Mean squared error for Arousal: 0.02\n",
      "Mean squared error for Dominance: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge regression - train dataset\")\n",
    "print(\"Mean squared error for Valance: %.2f\" % mean_squared_error(y_Vtrain, rslt_train_V_R))\n",
    "#print(\"Coefficient of determination Valence: %.2f\" % r2_score(y_Vtest, rslt_V))\n",
    "print(\"Mean squared error for Arousal: %.2f\" % mean_squared_error(y_Atrain, rslt_train_A_R))\n",
    "#print(\"Coefficient of determination Arousal: %.2f\" % r2_score(y_Atest, rslt_A))\n",
    "print(\"Mean squared error for Dominance: %.2f\" % mean_squared_error(y_Dtrain, rslt_train_D_R))\n",
    "#print(\"Coefficient of determination Dominance: %.2f\" % r2_score(y_Dtest, rslt_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0cc9de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression - train dataset\n",
      "Mean squared error for Valance: 0.00\n",
      "Mean squared error for Arousal: 0.00\n",
      "Mean squared error for Dominance: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear regression - train dataset\")\n",
    "print(\"Mean squared error for Valance: %.2f\" % mean_squared_error(y_Vtrain, rslt_train_V_L))\n",
    "#print(\"Coefficient of determination Valence: %.2f\" % r2_score(y_Vtest, rslt_V))\n",
    "print(\"Mean squared error for Arousal: %.2f\" % mean_squared_error(y_Atrain, rslt_train_A_L))\n",
    "#print(\"Coefficient of determination Arousal: %.2f\" % r2_score(y_Atest, rslt_A))\n",
    "print(\"Mean squared error for Dominance: %.2f\" % mean_squared_error(y_Dtrain, rslt_train_D_L))\n",
    "#print(\"Coefficient of determination Dominance: %.2f\" % r2_score(y_Dtest, rslt_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07cbde10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression - dev dataset\n",
      "Mean squared error for Valance: 0.09\n",
      "Mean squared error for Arousal: 0.06\n",
      "Mean squared error for Dominance: 0.04\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge regression - dev dataset\")\n",
    "print(\"Mean squared error for Valance: %.2f\" % mean_squared_error(y_Vdev, rslt_dev_V_R))\n",
    "#print(\"Coefficient of determination Valence: %.2f\" % r2_score(y_Vtest, rslt_V))\n",
    "print(\"Mean squared error for Arousal: %.2f\" % mean_squared_error(y_Adev, rslt_dev_A_R))\n",
    "#print(\"Coefficient of determination Arousal: %.2f\" % r2_score(y_Atest, rslt_A))\n",
    "print(\"Mean squared error for Dominance: %.2f\" % mean_squared_error(y_Ddev, rslt_dev_D_R))\n",
    "#print(\"Coefficient of determination Dominance: %.2f\" % r2_score(y_Dtest, rslt_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be4ced34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression - train dataset\n",
      "Mean squared error for Valance: 3557564.68\n",
      "Mean squared error for Arousal: 29836631.64\n",
      "Mean squared error for Dominance: 0.10\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear regression - train dataset\")\n",
    "print(\"Mean squared error for Valance: %.2f\" % mean_squared_error(y_Vdev, rslt_dev_V_L))\n",
    "#print(\"Coefficient of determination Valence: %.2f\" % r2_score(y_Vtest, rslt_V))\n",
    "print(\"Mean squared error for Arousal: %.2f\" % mean_squared_error(y_Adev, rslt_dev_A_L))\n",
    "#print(\"Coefficient of determination Arousal: %.2f\" % r2_score(y_Atest, rslt_A))\n",
    "print(\"Mean squared error for Dominance: %.2f\" % mean_squared_error(y_Ddev, rslt_dev_D_L))\n",
    "#print(\"Coefficient of determination Dominance: %.2f\" % r2_score(y_Dtest, rslt_D))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
