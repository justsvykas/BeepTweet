{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cb9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is to show experiments with Neural and Recursive Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b231dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eb = pd.read_csv(\"emobank.csv\", index_col=0 ,engine='python')\n",
    "eb.reset_index(drop=True, inplace=True)\n",
    "eb = eb.drop(labels='split', axis=1)\n",
    "eb_list = eb['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa11962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\justs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:438: MarkupResemblesLocatorWarning: \"http://www.law.com/jsp/nlj/PubArticleNLJ.jsp?id=1202463630848&Alcohol_bill_means_happy_hour_for_lobbyists\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "c:\\users\\justs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\bs4\\__init__.py:343: MarkupResemblesLocatorWarning: \".\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "# Based on https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "# Data cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "testing = eb_list\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc344f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_list_V = test_result\n",
    "eb_list_A = test_result\n",
    "eb_list_D = test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da98664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "# This function is taken from \n",
    "# https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31676664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature \n",
    "# Preprocessing function with Tf-idf\n",
    "# Based on code from:\n",
    "# https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts. This way no leaking of information is done.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e2d3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_V = eb['V']\n",
    "y_A = eb['A']\n",
    "y_D = eb['D']\n",
    "X_Vtrain, X_Vtest, y_Vtrain, y_Vtest = train_test_split(eb_list_V, y_V, test_size=0.3)\n",
    "X_Atrain, X_Atest, y_Atrain, y_Atest = train_test_split(eb_list_A, y_A, test_size=0.3)\n",
    "X_Dtrain, X_Dtest, y_Dtrain, y_Dtest = train_test_split(eb_list_D, y_D, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd59ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data with ngram\n",
    "#X_Vtrain, X_Vtest = ngram_vectorize(X_Vtrain, y_Vtrain, X_Vtest)\n",
    "#X_Atrain, X_Atest = ngram_vectorize(X_Atrain, y_Atrain, X_Atest)\n",
    "#X_Dtrain, X_Dtest = ngram_vectorize(X_Dtrain, y_Dtrain, X_Dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcdac341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data with embeddings\n",
    "X_Vtrain, X_Vtest, word_index_V, max_len_V = sequence_vectorize(X_Vtrain, X_Vtest)\n",
    "X_Atrain, X_Atest, word_index_A, max_len_A = sequence_vectorize(X_Atrain, X_Atest)\n",
    "X_Dtrain, X_Dtest, word_index_D, max_len_D = sequence_vectorize(X_Dtrain, X_Dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f349f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into dev set from training set so we get the same distribution\n",
    "X_Vtest, X_Vdev, y_Vtest, y_Vdev = train_test_split(X_Vtest, y_Vtest, test_size=0.5)\n",
    "X_Atest, X_Adev, y_Atest, y_Adev = train_test_split(X_Atest, y_Atest, test_size=0.5)\n",
    "X_Dtest, X_Ddev, y_Dtest, y_Ddev = train_test_split(X_Dtest, y_Dtest, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5a6f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VLAD - list to tensor\n",
    "import tensorflow as tf\n",
    "X_Vtrain_dataset = tf.data.Dataset.from_tensor_slices((X_Vtrain, y_Vtrain))\n",
    "X_Vtest_dataset = tf.data.Dataset.from_tensor_slices((X_Vtest, y_Vtest))\n",
    "X_Vdev_dataset = tf.data.Dataset.from_tensor_slices((X_Vdev, y_Vdev))\n",
    "\n",
    "X_Atrain_dataset = tf.data.Dataset.from_tensor_slices((X_Atrain, y_Atrain))\n",
    "X_Atest_dataset = tf.data.Dataset.from_tensor_slices((X_Atest, y_Atest))\n",
    "X_Adev_dataset = tf.data.Dataset.from_tensor_slices((X_Adev, y_Adev))\n",
    "X_Dtrain_dataset = tf.data.Dataset.from_tensor_slices((X_Dtrain, y_Dtrain))\n",
    "X_Dtest_dataset = tf.data.Dataset.from_tensor_slices((X_Dtest, y_Dtest))\n",
    "X_Ddev_dataset = tf.data.Dataset.from_tensor_slices((X_Ddev, y_Ddev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf1b021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((120,), ()), types: (tf.int32, tf.float64)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Vtrain_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43263507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VLAD - tensor to batch\n",
    "BUFFER_SIZE = 1000\n",
    "train_batches_V = (\n",
    "    X_Vtrain_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Vtrain_dataset)))\n",
    "\n",
    "test_batches_V = (\n",
    "    X_Vtest_dataset\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Vtest_dataset)))\n",
    "dev_batches_V = (\n",
    "    X_Vdev_dataset\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Vdev_dataset)))\n",
    "\n",
    "train_batches_A = (\n",
    "    X_Atrain_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Atrain_dataset)))\n",
    "\n",
    "test_batches_A = (\n",
    "    X_Atest_dataset\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Atest_dataset)))\n",
    "dev_batches_A = (\n",
    "    X_Adev_dataset\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Adev_dataset)))\n",
    "\n",
    "train_batches_D = (\n",
    "    X_Dtrain_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Dtrain_dataset)))\n",
    "\n",
    "test_batches_D = (\n",
    "    X_Dtest_dataset\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Dtest_dataset)))\n",
    "dev_batches_D = (\n",
    "    X_Ddev_dataset\n",
    "    .padded_batch(32, tf.compat.v1.data.get_output_shapes(X_Ddev_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "110be7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          222480    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222,497\n",
      "Trainable params: 222,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using linear activation function since this is regression\n",
    "\"\"\" Build the model \"\"\"\n",
    "def create_model() :\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index_V)+1, 16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation='linear')])  \n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  model.compile(optimizer='sgd',\n",
    "                loss=tf.keras.losses.MeanSquaredError())\n",
    "                 #metrics=['accuracy']) accuracy doesnt work for regression)\n",
    "  return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b52e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "207/207 [==============================] - 2s 4ms/step - loss: 0.5784 - val_loss: 0.1573\n",
      "Epoch 2/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1715 - val_loss: 0.1523\n",
      "Epoch 3/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1680 - val_loss: 0.1491\n",
      "Epoch 4/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1648 - val_loss: 0.1461\n",
      "Epoch 5/10\n",
      "207/207 [==============================] - 1s 4ms/step - loss: 0.1618 - val_loss: 0.1437\n",
      "Epoch 6/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1591 - val_loss: 0.1415\n",
      "Epoch 7/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1565 - val_loss: 0.1387\n",
      "Epoch 8/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1544 - val_loss: 0.1367\n",
      "Epoch 9/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1522 - val_loss: 0.1350\n",
      "Epoch 10/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.1504 - val_loss: 0.1338\n",
      "Valence training set error\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "history = model.fit(train_batches_V,\n",
    "                    epochs=10,\n",
    "                    validation_data=dev_batches_V,\n",
    "                    validation_steps=30)\n",
    "print(\"Valence training set error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2555c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0900 - val_loss: 0.0859\n",
      "Epoch 2/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0882 - val_loss: 0.0844\n",
      "Epoch 3/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0868 - val_loss: 0.0831\n",
      "Epoch 4/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0854 - val_loss: 0.0828\n",
      "Epoch 5/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0842 - val_loss: 0.0810\n",
      "Epoch 6/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0830 - val_loss: 0.0801\n",
      "Epoch 7/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0819 - val_loss: 0.0789\n",
      "Epoch 8/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0809 - val_loss: 0.0779\n",
      "Epoch 9/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0801 - val_loss: 0.0771\n",
      "Epoch 10/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0793 - val_loss: 0.0763\n",
      "Arousal training set error\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "history = model.fit(train_batches_A,\n",
    "                    epochs=10,\n",
    "                    validation_data=dev_batches_A,\n",
    "                    validation_steps=30)\n",
    "print(\"Arousal training set error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f685b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0556 - val_loss: 0.0577\n",
      "Epoch 2/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0548 - val_loss: 0.0568\n",
      "Epoch 3/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0540 - val_loss: 0.0561\n",
      "Epoch 4/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0533 - val_loss: 0.0555\n",
      "Epoch 5/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0526 - val_loss: 0.0549\n",
      "Epoch 6/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0520 - val_loss: 0.0544\n",
      "Epoch 7/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0514 - val_loss: 0.0539\n",
      "Epoch 8/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0509 - val_loss: 0.0534\n",
      "Epoch 9/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0504 - val_loss: 0.0530\n",
      "Epoch 10/10\n",
      "207/207 [==============================] - 1s 3ms/step - loss: 0.0499 - val_loss: 0.0526\n",
      "Dominance training set error\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "history = model.fit(train_batches_D,\n",
    "                    epochs=10,\n",
    "                    validation_data=dev_batches_D,\n",
    "                    validation_steps=30)\n",
    "print(\"Dominance training set error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72df56b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "model_rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index_V)+1, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34600a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_rnn.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer='sgd')\n",
    "                  #metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dc8b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "207/207 [==============================] - 28s 107ms/step - loss: 0.4212 - val_loss: 0.1140\n",
      "Epoch 2/10\n",
      "207/207 [==============================] - 17s 80ms/step - loss: 0.1266 - val_loss: 0.1125\n",
      "Epoch 3/10\n",
      "207/207 [==============================] - 15s 72ms/step - loss: 0.1267 - val_loss: 0.1129\n",
      "Epoch 4/10\n",
      "207/207 [==============================] - 17s 81ms/step - loss: 0.1265 - val_loss: 0.1104\n",
      "Epoch 5/10\n",
      "207/207 [==============================] - 17s 83ms/step - loss: 0.1264 - val_loss: 0.1108\n",
      "Epoch 6/10\n",
      "207/207 [==============================] - 17s 80ms/step - loss: 0.1264 - val_loss: 0.1112\n",
      "Epoch 7/10\n",
      "207/207 [==============================] - 17s 80ms/step - loss: 0.1265 - val_loss: 0.1157\n",
      "Epoch 8/10\n",
      "207/207 [==============================] - 17s 80ms/step - loss: 0.1259 - val_loss: 0.1108\n",
      "Epoch 9/10\n",
      "207/207 [==============================] - 17s 83ms/step - loss: 0.1263 - val_loss: 0.1102\n",
      "Epoch 10/10\n",
      "207/207 [==============================] - 18s 87ms/step - loss: 0.1260 - val_loss: 0.1110\n",
      "Valence training set error\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "history = model_rnn.fit(train_batches_V,\n",
    "                    epochs=10,\n",
    "                    validation_data=dev_batches_V,\n",
    "                    validation_steps=30)\n",
    "print(\"Valence training set error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa5f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
